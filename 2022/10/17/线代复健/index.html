
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>线性代数复健 - Exploring</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Peace,"> 
    <meta name="description" content="WJ&#39;s Learning Note,前言：最近在学习机器学习，但是在看AE(AutoEncoder)时，感觉到了其与PCA(Principal Component Analysis)的联系，从而对SVD(Singular Value ,"> 
    <meta name="author" content="Wang Jia"> 
    <link rel="alternative" href="atom.xml" title="Exploring" type="application/atom+xml"> 
    
    
    
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="线性代数复健 - Exploring"/>
    <meta name="twitter:description" content="WJ&#39;s Learning Note,前言：最近在学习机器学习，但是在看AE(AutoEncoder)时，感觉到了其与PCA(Principal Component Analysis)的联系，从而对SVD(Singular Value ,"/>
    
    
    
    
    <meta property="og:site_name" content="Exploring"/>
    <meta property="og:type" content="object"/>
    <meta property="og:title" content="线性代数复健 - Exploring"/>
    <meta property="og:description" content="WJ&#39;s Learning Note,前言：最近在学习机器学习，但是在看AE(AutoEncoder)时，感觉到了其与PCA(Principal Component Analysis)的联系，从而对SVD(Singular Value ,"/>
    
<link rel="stylesheet" href="/css/diaspora.css">

    <script>window.searchDbPath = "/search.xml";</script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap" rel="stylesheet">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head>

<body class="loading">
    <span id="config-title" style="display:none">Exploring</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="https://wjxdu.github.io"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">线性代数复健</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">线性代数复健</h1>
        <div class="stuff">
            <span>十月 17, 2022</span>
            

        </div>
        <div class="content markdown">
            <p>前言：最近在学习机器学习，但是在看AE(AutoEncoder)时，感觉到了其与PCA(Principal Component Analysis)的联系，从而对SVD(Singular Value Decomposition)产生了较大的兴趣。借此机会，对之前上过的高等代数课的部分知识做一个复健整理。</p>
<p>说到线代，我们首先能想到嘴边的概念有：向量，矩阵，秩，线性空间，线性变化，逆矩阵，单位矩阵，相似矩阵，特征值分解。核心还是矩阵，我们可以把矩阵看作一组基或者看作一个线性变换。</p>
<h3 id="基和坐标"><a href="#基和坐标" class="headerlink" title="基和坐标"></a>基和坐标</h3><p>例如，$\mathbf{x}=\left( x_1,x_2,,x_n \right) $代表一个向量，$X=\left( \begin{matrix}<br>    x_{11}&amp;        x_{12}&amp;        x_{13}\\<br>    x_{21}&amp;        x_{22}&amp;        x_{23}\\<br>    x_{31}&amp;        x_{32}&amp;        x_{33}\\<br>\end{matrix} \right) $代表一个矩阵，我们令$\mathbf{x}_1=\left( x_{11},x_{21},x_{31} \right) ^T<br>,<br>\mathbf{x}_2=\left( x_{12},x_{22},x_{32} \right) ^T<br>,<br>\mathbf{x}_3=\left( x_{13},x_{23},x_{33} \right) ^T$</p>
<p>则有$X=\left( \mathbf{x}_1,\mathbf{x}_2,\mathbf{x}_3 \right) $。即矩阵的每一列代表一个基。</p>
<p>我们常见的单位矩阵$I=\left( \begin{matrix}<br>    1&amp;        0&amp;        0\\<br>    0&amp;        1&amp;        0\\<br>    0&amp;        0&amp;        1\\<br>\end{matrix} \right) $即是把$\mathbf{\varepsilon }_1=\left( 1,0,0 \right) ,\mathbf{\varepsilon }_2=\left( 0,1,0 \right), \mathbf{\varepsilon }_3=\left( 0,0,1 \right)<br>$当作基。</p>
<p>当然，谈到矩阵的基，自然会回忆起线性无关，正交基，施密特正交化这一系列东西，但是这些知识还比较熟悉，不是复习的重点。给定一个向量$\alpha =\left( 1,2,3 \right) ^T$，我们可以把它看作一个向量，也可以把它看作一个坐标，即$\alpha =1\cdot \mathbf{\varepsilon }_1+2\cdot \mathbf{\varepsilon }_2+3\cdot \mathbf{\varepsilon }_3=\left( \mathbf{\varepsilon }_1,\mathbf{\varepsilon }_2,\mathbf{\varepsilon }_3 \right) \cdot \left( 1,2,3 \right) ^T=I\cdot \alpha<br>$，事实上，我们既然写成了这个样子，就潜意识地将标准基作为基底了。</p>
<h3 id="线性变换的引出"><a href="#线性变换的引出" class="headerlink" title="线性变换的引出"></a>线性变换的引出</h3><p>另外一种看法是将矩阵看成一种线性变换，确切来说，矩阵是某个线性变换在一组基上的表示。</p>
<p>按照课本的叙述方式：</p>
<p>设$V$是数域$F$上一个$n$维向量空间，令$\sigma$是$V$的一个线性变换。取定$V$的一个基</p>
<script type="math/tex; mode=display">
\mathbf{\alpha }_1,\mathbf{\alpha }_2,\cdots ,\mathbf{\alpha }_n</script><p>考虑$V$中任意一个向量</p>
<script type="math/tex; mode=display">
\xi =x_1\mathbf{\alpha }_1+x_2\mathbf{\alpha }_2+\cdots +x_n\mathbf{\alpha }_n</script><p>$\sigma \left( \xi \right)$仍是$V$的一个向量。设</p>
<script type="math/tex; mode=display">
\sigma \left( \xi \right) =y_1\mathbf{\alpha }_1+y_2\mathbf{\alpha }_2+\cdots +y_n\mathbf{\alpha }_n</script><p>自然要问，如何计算$\sigma \left( \xi \right)$的坐标$\left( y_1,y_2,\cdots ,y_n \right)$。</p>
<p>令</p>
<script type="math/tex; mode=display">
\begin{cases}
    \sigma \left( \mathbf{\alpha }_1 \right) =a_{11}\mathbf{\alpha }_1+a_{21}\mathbf{\alpha }_2+\cdots +a_{n1}\mathbf{\alpha }_n\\
    \sigma \left( \mathbf{\alpha }_2 \right) =a_{12}\mathbf{\alpha }_1+a_{22}\mathbf{\alpha }_2+\cdots +a_{n2}\mathbf{\alpha }_n\\
    \,\,                  \cdots \cdots \cdots \cdots\\
    \sigma \left( \mathbf{\alpha }_n \right) =a_{1n}\mathbf{\alpha }_1+a_{2n}\mathbf{\alpha }_2+\cdots +a_{nn}\mathbf{\alpha }_n\\
\end{cases}</script><p>这里$a_{ij}$就是$\sigma \left( \mathbf{\alpha }_j \right) $关于基$\mathbf{\alpha }_1,\mathbf{\alpha }_2,\cdots ,\mathbf{\alpha }_n$的坐标。</p>
<p>令</p>
<script type="math/tex; mode=display">
A=\left( \begin{matrix}
    a_{11}&        a_{12}&        \cdots&        a_{1n}\\
    a_{21}&        a_{22}&        \cdots&        a_{2n}\\
    \vdots&        \vdots&        &        \vdots\\
    a_{n1}&        a_{n2}&        \cdots&        a_{nn}\\
\end{matrix} \right)</script><p>$n$阶矩阵$A$叫做线性变换$\sigma$关于基$\left\{ \mathbf{\alpha }_1,\mathbf{\alpha }_2,\cdots ,\mathbf{\alpha }_n \right\} $的矩阵。矩阵$A$的第$j$列的元素就是$\sigma \left( \mathbf{\alpha }_j \right) $关于基$\left\{ \mathbf{\alpha }_1,\mathbf{\alpha }_2,\cdots ,\mathbf{\alpha }_n \right\} $的坐标。</p>
<p>这样，取定$F$上$n$维向量空间$V$的一个基后，对于$V$的每一线性变换，有唯一确定的$F$上$n$阶矩阵与之对应。</p>
<p>事实上：</p>
<script type="math/tex; mode=display">
\left( \sigma \left( \mathbf{\alpha }_1 \right) ,\sigma \left( \mathbf{\alpha }_2 \right) ,\cdots ,\sigma \left( \mathbf{\alpha }_n \right) \right) =\left( \mathbf{\alpha }_1,\mathbf{\alpha }_2,\cdots ,\mathbf{\alpha }_n \right) A</script><p>这意味着从基$\left\{ \mathbf{\alpha }_1,\mathbf{\alpha }_2,\cdots ,\mathbf{\alpha }_n \right\} $到基$\left\{ \sigma \left( \mathbf{\alpha }_1 \right) ,\sigma \left( \mathbf{\alpha }_2 \right) ,\cdots ,\sigma \left( \mathbf{\alpha }_n \right) \right\} $的过渡矩阵$A$即是线性变换$\sigma$关于基$\left\{ \mathbf{\alpha }_1,\mathbf{\alpha }_2,\cdots ,\mathbf{\alpha }_n \right\} $的矩阵。</p>
<h3 id="从拉伸角度理解线性变换"><a href="#从拉伸角度理解线性变换" class="headerlink" title="从拉伸角度理解线性变换"></a>从拉伸角度理解线性变换</h3><p>我们将坐标轴进行拉伸，即相当于对$\left( \begin{matrix}<br>    1&amp;        0\\<br>    0&amp;        1\\<br>\end{matrix} \right) $做变换  $\left( \begin{matrix}<br>    \lambda _1&amp;        0\\<br>    0&amp;        \lambda _2\\<br>\end{matrix} \right) $，即$\left( \begin{matrix}<br>    \lambda _1&amp;        0\\<br>    0&amp;        \lambda _2\\<br>\end{matrix} \right) \left( \begin{matrix}<br>    1&amp;        0\\<br>    0&amp;        1\\<br>\end{matrix} \right) =\left( \begin{matrix}<br>    \lambda _1&amp;        0\\<br>    0&amp;        \lambda _2\\<br>\end{matrix} \right) $，显然，对标准正交基做的拉伸这一类线性变换对应的矩阵都是对角矩阵。那么，当一个线性变换对于标准正交基的矩阵不是对角矩阵时，我们能不能有拉伸来理解呢？事实上，可以，这就是特征向量和特征值的一个很重要的意义。</p>
<p>我们先反过来看，假如有一个线性变换对于基$\left( \begin{matrix}<br>    1&amp;        -1\\<br>    0&amp;        1\\<br>\end{matrix} \right) $的矩阵是$B=\left( \begin{matrix}<br>    3&amp;        0\\<br>    0&amp;        2\\<br>\end{matrix} \right) $，那么我们只需要命名基$\left( \begin{matrix}<br>    1&amp;        0\\<br>    0&amp;        1\\<br>\end{matrix} \right) $到基$\left( \begin{matrix}<br>    1&amp;        -1\\<br>    0&amp;        1\\<br>\end{matrix} \right) $的过渡矩阵$T=\left( \begin{matrix}<br>    1&amp;        -1\\<br>    0&amp;        1\\<br>\end{matrix} \right) $，命名这一线性变换关于基$\left( \begin{matrix}<br>    1&amp;        0\\<br>    0&amp;        1\\<br>\end{matrix} \right) $的矩阵为$A$，根据相似矩阵的知识，有$B=T^{-1}AT$，据此求得$A=TBT^{-1}=\left( \begin{matrix}<br>    3&amp;        1\\<br>    0&amp;        2\\<br>\end{matrix} \right)$，所以，如果一个线性变换关于标准正交基的矩阵可以相似对角化，那么这个变换就可以被理解为对某组基的拉伸，这组基上的向量就是特征向量，对应的拉伸强度就是对应的特征值，至此，我们找到了一个更为直观的视角去看待线性变换。</p>
<p>书上给出了一个对角化的充要条件，即$n$维向量空间$V$的一个线性变换$\sigma$可以对角化的充要的条件是，$V$可以分解为$n$个在$\sigma$之下不变的一维子空间$W_1,W_2,\cdots ,W_n$的直和。我们可以简单地理解为，这一个线性变换不会让空间坍塌降维。</p>
<p>另外一点值得记录的是，这种拉伸是针对指定的方向的，并不是针对方向上的某一个向量，如果我们在选定的方向上选择标准基作为基，那么我们得到的特征向量矩阵将是一个正交矩阵，即它只表示方向。我们用$W$来表示特征向量矩阵，用对角阵$\varSigma$来表示拉伸强度，则有$A=W\varSigma W^{-1}$，因为我们选取的$W$为正交矩阵，所以有$WW^T=I$，所以$A=W\varSigma W^{T}$。</p>
<h3 id="非方阵"><a href="#非方阵" class="headerlink" title="非方阵"></a>非方阵</h3><p>我们将$\left( \begin{matrix}<br>    1&amp;        2\\<br>    -1&amp;        4\\<br>    3&amp;        0\\<br>\end{matrix} \right)$作用于$\left( \begin{matrix}<br>    1&amp;        0\\<br>    0&amp;        1\\<br>\end{matrix} \right)$，即$\left( \begin{matrix}<br>    1&amp;        2\\<br>    -1&amp;        4\\<br>    3&amp;        0\\<br>\end{matrix} \right) \left( \begin{matrix}<br>    1&amp;        0\\<br>    0&amp;        1\\<br>\end{matrix} \right) =\left( \begin{matrix}<br>    1&amp;        2\\<br>    -1&amp;        4\\<br>    3&amp;        0\\<br>\end{matrix} \right) $，根据很多视频的说法，这个矩阵将平面映射成了斜面，我的理解是，非方阵会将一基从$m$维映射到$n$维，但是由这些映射完的基所张成的空间的维度还是没有变大。</p>
<h3 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h3><p>根据这个文章的思路作为主线<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p>
<p>存在一个mxn的矩阵$A$，我们定义$A$的SVD为$A=U\varSigma V^{T}$，其中$U$是mxm的矩阵，$\varSigma$是mxn的矩阵，$V$是nxn的矩阵，我们可以理解为，$U$是在原平面找的特征向量矩阵，$V$是在变换后的斜面找的特征向量矩阵。</p>
<script type="math/tex; mode=display">
A^TA=V\varSigma ^TU^TU\varSigma V^T=V\varSigma ^2V^T
\\
AA^T=U\varSigma V^TV\varSigma ^TU^T=U\varSigma ^2U^T</script><p>所以我们可以根据$A$来求得$A^TA$和$AA^T$，进而求出$V,\varSigma,U$。</p>
<p>根据文中的例子：</p>
<script type="math/tex; mode=display">
A=\left( \begin{matrix}
    0&        1\\
    1&        1\\
    1&        0\\
\end{matrix} \right) ,A^T=\left( \begin{matrix}
    0&        1&        1\\
    1&        1&        0\\
\end{matrix} \right)</script><p>然后对方阵$A^TA$和$AA^T$进行分解。</p>
<script type="math/tex; mode=display">
A^TA=\left( \begin{matrix}
    0&        1&        1\\
    1&        1&        0\\
\end{matrix} \right) \left( \begin{matrix}
    0&        1\\
    1&        1\\
    1&        0\\
\end{matrix} \right) =\left( \begin{matrix}
    2&        1\\
    1&        2\\
\end{matrix} \right) =\left( \begin{matrix}
    \frac{1}{\sqrt{2}}&        -\frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{2}}\\
\end{matrix} \right) \left( \begin{matrix}
    3&        0\\
    0&        1\\
\end{matrix} \right) \left( \begin{matrix}
    \frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{2}}\\
    -\frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{2}}\\
\end{matrix} \right) 
\\
AA^T=\left( \begin{matrix}
    0&        1\\
    1&        1\\
    1&        0\\
\end{matrix} \right) \left( \begin{matrix}
    0&        1&        1\\
    1&        1&        0\\
\end{matrix} \right) =\left( \begin{matrix}
    1&        1&        0\\
    1&        2&        1\\
    0&        1&        1\\
\end{matrix} \right) =\left( \begin{matrix}
    \frac{1}{\sqrt{6}}&        \frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{3}}\\
    \frac{2}{\sqrt{6}}&        0&        -\frac{1}{\sqrt{3}}\\
    \frac{1}{\sqrt{6}}&        -\frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{3}}\\
\end{matrix} \right) \left( \begin{matrix}
    3&        0&        0\\
    0&        1&        0\\
    0&        0&        0\\
\end{matrix} \right) \left( \begin{matrix}
    \frac{1}{\sqrt{6}}&        \frac{2}{\sqrt{6}}&        \frac{1}{\sqrt{6}}\\
    \frac{1}{\sqrt{2}}&        0&        -\frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{3}}&        -\frac{1}{\sqrt{3}}&        \frac{1}{\sqrt{3}}\\
\end{matrix} \right)</script><p>所以得：</p>
<script type="math/tex; mode=display">
A=\left( \begin{matrix}
    \frac{1}{\sqrt{6}}&        \frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{3}}\\
    \frac{2}{\sqrt{6}}&        0&        -\frac{1}{\sqrt{3}}\\
    \frac{1}{\sqrt{6}}&        -\frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{3}}\\
\end{matrix} \right) \left( \begin{matrix}
    3&        0\\
    0&        1\\
    0&        0\\
\end{matrix} \right) \left( \begin{matrix}
    \frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{2}}\\
    -\frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{2}}\\
\end{matrix} \right)</script><p>细心的话可以发现$\det \left( U \right) =\left| \begin{matrix}<br>    \frac{1}{\sqrt{6}}&amp;        \frac{1}{\sqrt{2}}&amp;        \frac{1}{\sqrt{3}}\\<br>    \frac{2}{\sqrt{6}}&amp;        0&amp;        -\frac{1}{\sqrt{3}}\\<br>    \frac{1}{\sqrt{6}}&amp;        -\frac{1}{\sqrt{2}}&amp;        \frac{1}{\sqrt{3}}\\<br>\end{matrix} \right|=-6\ne 0$，但$A^TA$的最后一个特征值是零，其实就代表着三维空间中一个维度坍塌了，而这一切的根源则是$\det \left( AA^T \right) =\left| \begin{matrix}<br>    1&amp;        1&amp;        0\\<br>    1&amp;        2&amp;        1\\<br>    0&amp;        1&amp;        1\\<br>\end{matrix} \right|=0$，这激起了我对$AA^T$和$A^TA$之间的关系的兴趣。</p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title="0" data-url="http://link.hhtjim.com/163/425570952.mp3"></li>
                        
                    
                        
                            <li title="1" data-url="http://link.hhtjim.com/163/425570952.mp3"></li>
                        
                    
                </ul>
            
        </div>
        
        
    <div id="gitalk-container" class="comment link"
		data-enable="false"
        data-ae="false"
        data-ci=""
        data-cs=""
        data-r=""
        data-o=""
        data-a=""
        data-d="false"
    >查看评论</div>


    </div>
    
        <div class="side">
            <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E5%92%8C%E5%9D%90%E6%A0%87"><span class="toc-number">1.</span> <span class="toc-text">基和坐标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E7%9A%84%E5%BC%95%E5%87%BA"><span class="toc-number">2.</span> <span class="toc-text">线性变换的引出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E6%8B%89%E4%BC%B8%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2"><span class="toc-number">3.</span> <span class="toc-text">从拉伸角度理解线性变换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E6%96%B9%E9%98%B5"><span class="toc-number">4.</span> <span class="toc-text">非方阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="toc-number">5.</span> <span class="toc-text">奇异值分解</span></a></li></ol>
        </div>
    
</div>


    </div>
</div>
</body>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




</html>
